{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vy3HBF_wzydG",
        "outputId": "8f6a2aae-4fbd-4806-949b-3a2a83ced097"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ysFUFbXa1J9g",
        "outputId": "f372e13b-f3db-4eef-97d0-5631f0db4817"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/karan/Developer/ChaiBachai\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "HOME = os.getcwd()\n",
        "print(HOME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ZV0RK_h1L-E",
        "outputId": "e7adff7e-2e37-4f54-9ec7-b5a9d2c4f0b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ultralytics 8.3.40 ðŸš€ Python-3.12.4 torch-2.2.2 CPU (Apple M2)\n",
            "Setup complete âœ… (8 CPUs, 24.0 GB RAM, 372.6/926.4 GB disk)\n"
          ]
        }
      ],
      "source": [
        "%pip install \"ultralytics<=8.3.40\" supervision roboflow\n",
        "import ultralytics\n",
        "ultralytics.checks()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Ob4vwZUWoMPO"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "import os\n",
        "import time\n",
        "import glob\n",
        "from IPython.display import Image as IPyImage, display\n",
        "import cv2\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VlCPERID1Nqe",
        "outputId": "2209a5df-2e50-4411-e4bb-5e3616fb1d42"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# prompt: give command to fetch a file from my google drive\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[1;32m      4\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google'"
          ]
        }
      ],
      "source": [
        "# prompt: give command to fetch a file from my google drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "vy8zMJ78ZMeZ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/karan/Developer/ChaiBachai\n",
            "/Users/karan/Developer/ChaiBachai/best-pot-detection-yolo11s-6.pt\n"
          ]
        }
      ],
      "source": [
        "# trainedModel=f\"{HOME}/drive/MyDrive/Tea-boiling-over/best-pot-detection-yolo11s-6.pt\"\n",
        "# testFolder = f\"{HOME}/drive/MyDrive/Tea-boiling-over/test-images\"\n",
        "HOME=os.getcwd()\n",
        "print(HOME)\n",
        "testImages=f\"{HOME}/test-images\"\n",
        "trainedModel=f\"{HOME}/best-pot-detection-yolo11s-6.pt\"\n",
        "print(trainedModel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MPS is available\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "if not torch.backends.mps.is_available():\n",
        "    if not torch.backends.mps.is_built():\n",
        "        print(\"MPS not available because the current PyTorch install was not \"\n",
        "              \"built with MPS enabled.\")\n",
        "    else:\n",
        "        print(\"MPS not available because the current MacOS version is not 12.3+ \"\n",
        "              \"and/or you do not have an MPS-enabled device on this machine.\")\n",
        "else:\n",
        "    print(\"MPS is available\")\n",
        "    mps_device = torch.device(\"mps\")\n",
        "    model=YOLO(trainedModel)\n",
        "    model.to(mps_device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5s4BhQedeKK",
        "outputId": "7e6b9daf-cf63-4081-f2b1-c0e0baf3474f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "ename": "NotImplementedError",
          "evalue": "The operator 'torchvision::nms' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[28], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPYTORCH_ENABLE_MPS_FALLBACK\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      4\u001b[0m model\u001b[38;5;241m=\u001b[39mYOLO(trainedModel)\n\u001b[0;32m----> 6\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtestImages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m results:\n\u001b[1;32m      9\u001b[0m     boxes \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mboxes  \u001b[38;5;66;03m# Boxes object for bbox outputs\u001b[39;00m\n",
            "File \u001b[0;32m~/Developer/ChaiBachai/venv/lib/python3.12/site-packages/ultralytics/engine/model.py:179\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    152\u001b[0m     source: Union[\u001b[38;5;28mstr\u001b[39m, Path, \u001b[38;5;28mint\u001b[39m, Image\u001b[38;5;241m.\u001b[39mImage, \u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray, torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    153\u001b[0m     stream: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    155\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    156\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03m    Alias for the predict method, enabling the model instance to be callable for predictions.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;124;03m        ...     print(f\"Detected {len(r)} objects in image\")\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Developer/ChaiBachai/venv/lib/python3.12/site-packages/ultralytics/engine/model.py:557\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_prompts\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mset_prompts(prompts)\n\u001b[0;32m--> 557\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Developer/ChaiBachai/venv/lib/python3.12/site-packages/ultralytics/engine/predictor.py:173\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[0;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Developer/ChaiBachai/venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:35\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m---> 35\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
            "File \u001b[0;32m~/Developer/ChaiBachai/venv/lib/python3.12/site-packages/ultralytics/engine/predictor.py:266\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[0;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# Postprocess\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m2\u001b[39m]:\n\u001b[0;32m--> 266\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpostprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mim0s\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_callbacks(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_predict_postprocess_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    269\u001b[0m \u001b[38;5;66;03m# Visualize, save, write results\u001b[39;00m\n",
            "File \u001b[0;32m~/Developer/ChaiBachai/venv/lib/python3.12/site-packages/ultralytics/models/yolo/detect/predict.py:25\u001b[0m, in \u001b[0;36mDetectionPredictor.postprocess\u001b[0;34m(self, preds, img, orig_imgs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpostprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, preds, img, orig_imgs):\n\u001b[1;32m     24\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Post-processes predictions and returns a list of Results objects.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnon_max_suppression\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miou\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43magnostic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magnostic_nms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_det\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_det\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclasses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclasses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(orig_imgs, \u001b[38;5;28mlist\u001b[39m):  \u001b[38;5;66;03m# input images are a torch.Tensor, not a list\u001b[39;00m\n\u001b[1;32m     35\u001b[0m         orig_imgs \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mconvert_torch2numpy_batch(orig_imgs)\n",
            "File \u001b[0;32m~/Developer/ChaiBachai/venv/lib/python3.12/site-packages/ultralytics/utils/ops.py:291\u001b[0m, in \u001b[0;36mnon_max_suppression\u001b[0;34m(prediction, conf_thres, iou_thres, classes, agnostic, multi_label, labels, max_det, nc, max_time_img, max_nms, max_wh, in_place, rotated)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    290\u001b[0m     boxes \u001b[38;5;241m=\u001b[39m x[:, :\u001b[38;5;241m4\u001b[39m] \u001b[38;5;241m+\u001b[39m c  \u001b[38;5;66;03m# boxes (offset by class)\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m     i \u001b[38;5;241m=\u001b[39m \u001b[43mtorchvision\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miou_thres\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# NMS\u001b[39;00m\n\u001b[1;32m    292\u001b[0m i \u001b[38;5;241m=\u001b[39m i[:max_det]  \u001b[38;5;66;03m# limit detections\u001b[39;00m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;66;03m# # Experimental\u001b[39;00m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;66;03m# merge = False  # use merge-NMS\u001b[39;00m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;66;03m# if merge and (1 < n < 3E3):  # Merge NMS (boxes merged using weighted mean)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;66;03m#     if redundant:\u001b[39;00m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;66;03m#         i = i[iou.sum(1) > 1]  # require redundancy\u001b[39;00m\n",
            "File \u001b[0;32m~/Developer/ChaiBachai/venv/lib/python3.12/site-packages/torchvision/ops/boxes.py:41\u001b[0m, in \u001b[0;36mnms\u001b[0;34m(boxes, scores, iou_threshold)\u001b[0m\n\u001b[1;32m     39\u001b[0m     _log_api_usage_once(nms)\n\u001b[1;32m     40\u001b[0m _assert_has_ops()\n\u001b[0;32m---> 41\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtorchvision\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miou_threshold\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Developer/ChaiBachai/venv/lib/python3.12/site-packages/torch/_ops.py:755\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    750\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    751\u001b[0m     \u001b[38;5;66;03m# overloading __call__ to ensure torch.ops.foo.bar()\u001b[39;00m\n\u001b[1;32m    752\u001b[0m     \u001b[38;5;66;03m# is still callable from JIT\u001b[39;00m\n\u001b[1;32m    753\u001b[0m     \u001b[38;5;66;03m# We save the function ptr as the `op` attribute on\u001b[39;00m\n\u001b[1;32m    754\u001b[0m     \u001b[38;5;66;03m# OpOverloadPacket to access it here.\u001b[39;00m\n\u001b[0;32m--> 755\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: The operator 'torchvision::nms' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
          ]
        }
      ],
      "source": [
        "# prompt: how to extract the bounding box dimensions from the images\n",
        "# !yolo task=detect mode=predict model={trainedModel} conf=0.5 source={testFolder} save=True\n",
        "os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n",
        "model=YOLO(trainedModel)\n",
        "\n",
        "results = model(source=testImages, save=True, device=\"mps\")\n",
        "\n",
        "for r in results:\n",
        "    boxes = r.boxes  # Boxes object for bbox outputs\n",
        "    for box in boxes:\n",
        "        b = box.xyxy[0]  # get box coordinates in (top, left, bottom, right) format\n",
        "        width = b[2] - b[0]\n",
        "        height = b[3] - b[1]\n",
        "\n",
        "        print(f\"Bounding box dimensions: Width = {width:.2f}, Height = {height:.2f}\")\n",
        "        # You can access other information from box object like confidence score, class id etc.\n",
        "        print(box.conf) # confidence score\n",
        "        print(box.cls)  # class id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQlDTYxaaU8T",
        "outputId": "1a441844-d3ba-4ca4-bc5d-2097cbabb384"
      },
      "outputs": [],
      "source": [
        "# prompt: wrap this line to show time a performance\n",
        "start_time = time.time()\n",
        "!yolo task=detect mode=predict model={trainedModel} conf=0.5 source={testFolder} save=True\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "print(f\"Elapsed time: {elapsed_time:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "Ll_gnEo_YbOq"
      },
      "outputs": [],
      "source": [
        "# @title Default title text\n",
        "# prompt: You're absolutely correct that some pots have small notches or spouts, making the rim deviate from a perfect circle. Relying solely on Hough Circle Transform in such cases can lead to inaccuracies. Here's how you can adapt your approach to account for these irregularities:\n",
        "# 1. Augment Hough Circle Detection with Robust Methods\n",
        "# While Hough Circle Transform is useful for approximate detection, you can enhance it to handle non-circular rims:\n",
        "#     Segment the Rim Region Only:\n",
        "#         Focus on detecting the rim by masking the pot's top portion after detecting the bounding box with YOLOv11s.\n",
        "#         Use the upper 20â€“30% of the bounding box as the region for rim detection.\n",
        "# SAmple code mask = np.zeros_like(edges)\n",
        "# mask[y_min:int(y_min + 0.3 * box_height), x_min:x_max] = 255\n",
        "# rim_edges = cv2.bitwise_and(edges, edges, mask=mask)\n",
        "def detect_rim_with_hough_and_contours(image,boxes):\n",
        "    detections = []\n",
        "    for box in boxes:\n",
        "        x_min, y_min, x_max, y_max = box\n",
        "\n",
        "        # Crop the region of interest (ROI) around the pot\n",
        "        ROI = image[y_min:y_max, x_min:x_max]\n",
        "        # Convert ROI to grayscale and apply Gaussian blur\n",
        "        gray = cv2.cvtColor(ROI, cv2.COLOR_BGR2GRAY)\n",
        "        blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
        "        # Edge detection using Canny\n",
        "        edges = cv2.Canny(blurred, 50, 150)\n",
        "\n",
        "        # Mask the top portion of the pot (top 30% of ROI)\n",
        "        mask = np.zeros_like(edges)\n",
        "        box_height = y_max - y_min\n",
        "        top_mask_height = int(0.3 * box_height)\n",
        "        mask[:top_mask_height, :] = 255\n",
        "        rim_edges = cv2.bitwise_and(edges, edges, mask=mask)\n",
        "\n",
        "        # Apply Hough Circle Transform\n",
        "        circles = cv2.HoughCircles(\n",
        "            rim_edges,\n",
        "            cv2.HOUGH_GRADIENT,dp=1.2,minDist=20,param1=50,param2=30,\n",
        "            minRadius=10,  # Adjust based on expected pot sizes\n",
        "            maxRadius=100)\n",
        "\n",
        "        # Initialize variables\n",
        "        rim_line = None\n",
        "        output_image = ROI.copy()\n",
        "\n",
        "        if circles is not None:\n",
        "            # Convert circle coordinates to integers\n",
        "            circles = np.uint16(np.around(circles))\n",
        "            for circle in circles[0, :]:\n",
        "                x, y, r = circle\n",
        "                # Draw the detected circle\n",
        "                cv2.circle(output_image, (x, y), r, (0, 255, 0), 2)\n",
        "                rim_line = int(y - r)  # Top edge of the circle\n",
        "                break  # Use the first detected circle\n",
        "\n",
        "        # If no circle is detected, fall back to contour-based detection\n",
        "        if rim_line is None:\n",
        "            # Find contours in the edge-detected image\n",
        "            contours, _ = cv2.findContours(rim_edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "            if contours:\n",
        "                # Find the largest contour\n",
        "                largest_contour = max(contours, key=cv2.contourArea)\n",
        "                # Fit a bounding rectangle to the contour and use the top edge\n",
        "                x, y, w, h = cv2.boundingRect(largest_contour)\n",
        "                rim_line = y  # Top of the bounding box\n",
        "                # Draw the contour and bounding rectangle\n",
        "                cv2.drawContours(output_image, [largest_contour], -1, (255, 0, 0), 2)\n",
        "                cv2.rectangle(output_image, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
        "\n",
        "        # Draw the detected rim line on the output image\n",
        "        if rim_line is not None:\n",
        "            cv2.line(output_image, (0, rim_line), (output_image.shape[1], rim_line), (0, 0, 255), 2)\n",
        "\n",
        "        detections.append(rim_line, output_image)\n",
        "    return detections\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pzy01lR1kcEE",
        "outputId": "949cbc5d-face-4d50-ad32-d67e470c58da"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "latest_folder = max(glob.glob(f'{testImages}'), key=os.path.getmtime)\n",
        "for imgPath in glob.glob(f'{latest_folder}/*.jpg')[:]:\n",
        "    # Load an image (replace 'pot_image.jpg' with your test image)\n",
        "    image = cv2.imread(imgPath)\n",
        "    # Example bounding box coordinates from YOLOv11s model\n",
        "    model=YOLO(trainedModel)\n",
        "    results = model(image,save=True, imgsz=256, conf=0.5)\n",
        "    # result = model(image,save=True, imgsz=256, conf=0.5, device=\"mps\")\n",
        "\n",
        "    pot_bboxes=[]\n",
        "    for result in results:\n",
        "        pot_bboxees=result.boxes.xyxy\n",
        "\n",
        "    # Detect the rim\n",
        "    detections = detect_rim_with_hough_and_contours(image, pot_bboxes)\n",
        "    print(\"det\",len(detections))\n",
        "    for detection in detections:\n",
        "        print(\"\\n\\n\\ndetections\")\n",
        "        print(detection)\n",
        "        rim_line, output_image = detection\n",
        "        # Display the result\n",
        "        cv2.imshow(\"Detected Rim\", output_image)\n",
        "\n",
        "    cv2.waitKey(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KCFA1hO2kb8s"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SOtmFnmndJTf",
        "outputId": "d70124ff-068d-4143-e170-5fca5d8a0d73"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "latest_folder = max(glob.glob(f'{HOME}/runs/detect/predict*/'), key=os.path.getmtime)\n",
        "for img in glob.glob(f'{latest_folder}/*.jpg')[:]:\n",
        "    display(IPyImage(filename=img, width=600))\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "verQzgQqd5AS",
        "outputId": "a6d390e5-1721-48b8-fecc-1ecfa71d55f2"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def detect_rim_with_hough_and_contours(image, pot_bbox):\n",
        "    \"\"\"\n",
        "    Detects the rim of a pot using Hough Circle Transform augmented with contour analysis.\n",
        "\n",
        "    Args:\n",
        "        image: Input image (BGR).\n",
        "        pot_bbox: Tuple containing bounding box coordinates of the pot (x_min, y_min, x_max, y_max).\n",
        "\n",
        "    Returns:\n",
        "        rim_line: Y-coordinate of the detected rim.\n",
        "        output_image: Image with rim detection drawn.\n",
        "    \"\"\"\n",
        "    # Extract bounding box coordinates\n",
        "    x_min, y_min, x_max, y_max = pot_bbox\n",
        "\n",
        "    # Crop the region of interest (ROI) around the pot\n",
        "    ROI = image[y_min:y_max, x_min:x_max]\n",
        "\n",
        "    # Convert ROI to grayscale and apply Gaussian blur\n",
        "    gray = cv2.cvtColor(ROI, cv2.COLOR_BGR2GRAY)\n",
        "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
        "\n",
        "    # Edge detection using Canny\n",
        "    edges = cv2.Canny(blurred, 50, 150)\n",
        "\n",
        "    # Mask the top portion of the pot (top 30% of ROI)\n",
        "    mask = np.zeros_like(edges)\n",
        "    box_height = y_max - y_min\n",
        "    top_mask_height = int(0.3 * box_height)\n",
        "    mask[:top_mask_height, :] = 255\n",
        "    rim_edges = cv2.bitwise_and(edges, edges, mask=mask)\n",
        "\n",
        "    # Apply Hough Circle Transform\n",
        "    circles = cv2.HoughCircles(\n",
        "        rim_edges,\n",
        "        cv2.HOUGH_GRADIENT,\n",
        "        dp=1.2,\n",
        "        minDist=20,\n",
        "        param1=50,\n",
        "        param2=30,\n",
        "        minRadius=10,  # Adjust based on expected pot sizes\n",
        "        maxRadius=100\n",
        "    )\n",
        "\n",
        "    # Initialize variables\n",
        "    rim_line = None\n",
        "    output_image = ROI.copy()\n",
        "\n",
        "    if circles is not None:\n",
        "        # Convert circle coordinates to integers\n",
        "        circles = np.uint16(np.around(circles))\n",
        "        for circle in circles[0, :]:\n",
        "            x, y, r = circle\n",
        "            # Draw the detected circle\n",
        "            cv2.circle(output_image, (x, y), r, (0, 255, 0), 2)\n",
        "            rim_line = int(y - r)  # Top edge of the circle\n",
        "            break  # Use the first detected circle\n",
        "\n",
        "    # If no circle is detected, fall back to contour-based detection\n",
        "    if rim_line is None:\n",
        "        # Find contours in the edge-detected image\n",
        "        contours, _ = cv2.findContours(rim_edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "        if contours:\n",
        "            # Find the largest contour\n",
        "            largest_contour = max(contours, key=cv2.contourArea)\n",
        "            # Fit a bounding rectangle to the contour and use the top edge\n",
        "            x, y, w, h = cv2.boundingRect(largest_contour)\n",
        "            rim_line = y  # Top of the bounding box\n",
        "            # Draw the contour and bounding rectangle\n",
        "            cv2.drawContours(output_image, [largest_contour], -1, (255, 0, 0), 2)\n",
        "            cv2.rectangle(output_image, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
        "\n",
        "    # Draw the detected rim line on the output image\n",
        "    if rim_line is not None:\n",
        "        cv2.line(output_image, (0, rim_line), (output_image.shape[1], rim_line), (0, 0, 255), 2)\n",
        "\n",
        "    return rim_line, output_image\n",
        "\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Load an image (replace 'pot_image.jpg' with your test image)\n",
        "    image = cv2.imread(\"pot_image.jpg\")\n",
        "\n",
        "    # Example bounding box coordinates from YOLOv11s model\n",
        "    pot_bbox = (50, 100, 300, 400)  # Replace with actual YOLO output\n",
        "\n",
        "    # Detect the rim\n",
        "    rim_line, output_image = detect_rim_with_hough_and_contours(image, pot_bbox)\n",
        "\n",
        "    # Display the result\n",
        "    cv2.imshow(\"Detected Rim\", output_image)\n",
        "    cv2.waitKey(0)\n",
        "    cv2.destroyAllWindows()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOscAOtk4JcIL1GS2ZIUa0/",
      "mount_file_id": "1LlELUJ20MMEABrgHfG_d0FXTTzOEwTjl",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
